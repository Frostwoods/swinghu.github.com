---
layout: post
title: GPFS文件系统
description: 理解GPFS文件系统工作原理，设计理念，如何容错，同步？
category: blog
---


# GPFS: A Shared-Disk File System for Large Computing Clusters #


**1 总述**
GPFS是IMB公司为分布式计算机开发的并行,共享文件系统,它很大程度上借鉴了当时学术上提出的新的idea,尤其是分布式锁协议和恢复技术,其由Tiger Shark系统演化而来.Tiger Shark是由IBM公司Almaden研究中心为AIX操作系统设计的并行文件系统，约1993年的时候完成。它被设计用于支持大规模实时交互式多媒体应用，如交互电视（interactive television, ITV）。基于Tiger Shark文件系统，可以构建大规模的视频服务器，并能以每秒6Mb的速度传递几百个并行的MPEG流。Tiger Shark文件系统已经应用到了RS/6000的完整的产品线上，从最小的桌面机到SP-2并行超级计算机。GPFS以伸缩性著称,其高度的伸缩性主要来自他的共享磁盘架构:
![](http://i.imgur.com/6qOPaJX.png)

**2 数据分割，预分配，预取，回写**
GPFS支持4096个每个容量达 1TB的磁盘，每个文件系统可以达到4 petabytes。产品中最大的单个GPFS文件系统是75TB，机器是ASCI White。GPFS支持64位文件接口。虽然它对大文件系统的支持不是只针对Linux集群的，但是数据结构和算法还是值得讨论的。块的大小相同，连续的块分布在不同的磁盘上（循环的方式，例如取模）,连接文件系统节点的交换结构可以由一个存储区域网(storage area network)组成,例如iSCSI(后来扩展到IP网络：iSCSI是SAN内部实现的一种方式，就是用网络连接磁盘阵列)。大目录支持方面使用扩展hash来组织目录项。在大的文件系统里面每当文件被挂载或者每次一个集群中单个节点down掉就去执行一次fsck文件系统一致性检查，这是不可行的。GPFS文件系统的做法：记录了所有那些对文件系统一致性有影响的元数据，存入一个journal 文件里面或者采用先写日志，后写数据方式。预先写记录log日志。用户的数据没有被记录。

**分布式锁和集中管理**

节点是平等，两个不同节点上的进程访问同一个文件，其中的某个节点读文件，将会看到那么所有的另外一个节点所执行的写操作的数据，要么什么也看不到（因为write/read的原子性）。唯一一个例外是访问时间的更新上，这个更新不会在其他所有节点上立即可见。GPFS文件系统使用的分布式锁两种形式：

+ 分布式锁：每个文件系统操作都拥有适当的锁来同步
+ 通过指定某个节点进行集中管理

** 3.2锁管理器 **

&ensp;&ensp;&ensp;&ensp;锁组成：由集群中某个节点充当全局锁管理器加上其他每个文件系统节点处的局部锁管理器一同管理。全局锁管理器通过分发锁token来协调节点之间的局部锁

**3.3并行数据访问 **
&ensp;&ensp;&ensp;&ensp;需求：某些超级计算机要求从多个节点同时对同一个文件进行写入操作。GPFS提供了字节级别（byte-range locking）的锁来同步（synchronize）文件的读写操作。这就允许并发程序同时写文件的不同部分，同时还保持了POSIX协议的读写原子性语义。传统的字节级别的锁，使用方法原始，在读写过程的调用中，获取字节范围的token，并在完成后释放，锁开销难以承受。GPFS使用的是一种更加复杂的字节范围的锁协议，其从根本上减少了对于很多常用的访问模式的锁开销。

字节范围的锁令牌裁决机制：第一个对文件进行写操作的节点将会获得对整个文件的字节范围的锁（0-无穷大）。只要没有其他节点访问这一文件，所有的读写操作将会在本地进行，而不需要过多的节点之间的交互。当第二个节点开始写这个文件的时候，它将会撤销，至少部分第一个节点所持有的锁。当第一个节点接到撤销请求的时候，就会进行自我检查，检查文件是否仍然在使用。


+ 如果文件已经被关闭了，第一个节点就会放弃整个令牌，第二个节点将会能够获取令牌来补位。因此，没有并发写共享机制，GPFS文件系统中字节范围锁的行为就像整个文件锁一样，同样的高效（因为，一个单一的令牌交换就可以高效的访问整个文件）
+ 如果文件没有被关闭，也就是第二个节点在请求写的时候，第一个节点还没有关闭该文件。那么第一个节点将会只放弃部分锁，如果第一个节点正在偏移量O1处进行顺序性的写数据，第二个节点在O2位置处写。那么如果（1）O2 > O1,第一个节点将会释放O2到无穷大范围的字节锁。（2）O1 > O2,第一个节点将会释放从0到O1范围处的锁。这就使得两个节点同时在彼此的偏移量处写文件，而不会造成令牌冲突。

一般情况下，多个节点在写同一个文件的时候，在偏移量没有重叠的区域时，写操作前，每个节点将都能够获取到相关的锁，并且只需要一次令牌交换。节点请求写的范围信息以及期望的范围（将来期望写的位置），也会在令牌裁决传递这一环节附上,这些信息，对应于write 系统调用使用偏移量offset 和length。序列化写，期望范围就是当前写的位置到无穷。令牌协议将会撤销那些节点期望范围有冲突的的节点字节范围。令牌管理由令牌服务器授权管理（字节范围令牌服务器负责分割调整字节令牌的范围大小）。

整个GPFS字节范围锁协议达到的效果就是，当所有节点读写同一个文件和所有节点读写不同文件时I/O吞吐量基本一致.（注意所有节点读写同一个文件，文件被分割成n个连续的块，每个块一个对应一个读写操作节点node）GPFS所达成的读写吞吐量和纯粹的磁盘读写相匹配。通俗一点讲，就是从多个节点写同一个文件速度和每个节点写不同文件一样快，这就是字节范围锁协议所要表达的内容（byte-range token protocol ）。这种协议不仅仅适用于简单的顺序访问，还适合逆序访问，前向或者后向跳跃式访问。

同时字节范围锁协议也不是万能的，与此同时GPFS也采用了数据运送方式（Data shipping） Data shipping 主要运用在MPI/IO 库中，其不需要POSIX语义，提供了一种较为自然的机制给节点分配块。

**元数据的同步访问**

和其他文件系统的相同点：GPFS使用inodes和间接块来存储文件属性和数据块地址。
传统分布式文件系统，多个节点同时写同一个文件，会导致并发更新文件inode和间接块，例如，文件大小，修改时间，记录新分配的存储数据块地址。通过额外的inode节点的写锁来同步更新元数据，将会有可能导致在每次写操作的时候锁冲突。

由此GPFS文件系统使用inode节点上的共享写锁机制，这种机制允许并发的写多个节点。这种共享写锁，只有在请求确切文件大小和mtime（例如一个stat()系统调用或者，一个尝试读取文件超过末尾的读操作）调用时会冲突。当一个节点访问一个文件的时候，就会被指派为一个metanode,只有metanode读处，写入inode数据到硬盘。当共享写被撤销之后，metanode 就会合并其接收到的其他访问节点的inode更新数据。间接块的同步与此类似，当写入一个新文件的时候，每个节点都会独立的为它所写数据块分配磁盘空间，使用字节范围的令牌机制确保了任何某一数据块都会只有一个节点对其分配存储空间。

某一特殊文件的metanode是在token server的帮助下动态的选举出来的。
例如一个节点第一次访问某个文件，他就会尝试获得metanode令牌。与此同时，令牌会被授权给第一个访问的节点，相反，其他节点会知道这个节点的身份。当metanode不在访问该文件后，或者cache过期，这个节点就会放弃metanode令牌。停止行使metanode的权利，在此之后，如果他收到一个metanode请求，他会给出一个否定回复。其他节点就可以获取metanode令牌而成为新的metanode.

GPFS文件系统集群中，其中有一个节点负责所有分区空区域的统计。这一节点被称为：分配管理节点。当文件系统被挂载时，它通过读取 allocation map来初始化空磁盘空间的统计。

**其他文件系统元数据**
诸如：文件系统配置数据，空间使用限额，访问控制列表，扩展属性。GPFS使用分布式锁来保证磁盘上的元数据一致性，但是在大多数情况下还是使用集中管理的方式来协调和收集所需更新的元数据。

**令牌管理缩放（Token Manager Scaling）**


&ensp;&ensp;&ensp;&ensp;令牌管理跟踪者整个集群中所有节点的所有锁令牌的授权。节点请求，释放，升级，降级一个令牌需要与令牌管理进行一次通信。可想而知，令牌管理可能成为大集群中的一个瓶颈。实验数据显示由于令牌冲突导致的磁盘I/O花费是占据令牌管理通信的主要部分

&ensp;&ensp;&ensp;&ensp;一个可能解决这些问题的是对令牌空间进行分割并且将集群中的所有节点的令牌状态进行分布式到几个节点处。但是发现这并不是最好的方式，或者说不是解决令牌管理缩放的问题最关键的方式。一种直接方式就是：


+ 对节点之间的令牌状态进行分布式，通过hash算法到文件的inode编号number上。
+ 另外一种减少令牌管理负担，改善性能的更高效的方式，就是一开始就避免令牌冲突的发生。

GPFS文件系统采取的优化方法，如下：

&ensp;&ensp;&ensp;&ensp;当一个节点要放弃（撤销）令牌的时候，有撤销令牌节点发送撤销信息给所有那些在冲突模式下持有令牌的节点，然后收集这些节点的反馈，并对这些反馈进行处理成一条信息，然后发送给令牌管理器。由此获取一次令牌，不管有多少个冲突节点拥有令牌，通信量就不会超过两条信息。

同样GPFS对该协议进行优化，使得支持预存取和令牌请求批处理。可以一次通信从令牌管理器那里获得多个令牌。

**4.容错**
+ 节点容错
&ensp;&ensp;&ensp;&ensp;GPFS采用将恢复日志log存储在一块共享磁盘上，由于某个失败节点导致元数据的不一致性可以很快的被某个存活节点依据失败节点的日志运行log恢复进行修复。恢复完成后，令牌管理器释放失败节点所持有的令牌。分布式锁协议保证失败节点必须持有令牌，当cache中有元数据更新，但是这些元数据没有写回到到磁盘的时候。这些令牌只有在log恢复完成后。失败节所修改的元数据只有达到一致性的状态的时候才可以被其他节点访问。当日志恢复完成后，其他节点就可以获取失败节点元数据令牌（metanode token）并充当元数据节点的角色。其次，令牌管理器节点也有可能失败，另外一个新的节点将会充当令牌管理器的职责，通过查询所有存活节点当前持有的令牌，重新构建令牌管理。由于新的令牌管理器不知道失败节点所有持有的令牌，所有令牌管理器将不会授权任何其它新的令牌给它，直到日志log恢复完成以后。当前存活节点所持有的锁也不会受到此影响。其他，诸如如果失败节点的角色是allocation 管理节点的话，也是类似的。


+ 通信容错
&ensp;&ensp;&ensp;&ensp;通过周期性的发送心跳消息和组成员协议来检测失败节点。当某个节点失败，组成员服务层就会通知其他存活的节点。网络适配器和电缆可能会使得节点变得孤立，网络分区。不同分区的节点可能仍然拥有访问共享磁盘权限，如果他们还能够继续独立各自操作共享文件，这将会破坏文件系统。因此，GPFS文件系统只允许那些个拥有集群中绝大多数节点的组访问文件系统，而拥有小数节点的组将会被禁止访问磁盘，直到他们重新加入大组中。在节点出现错误，开始进行日志恢复之前，GPFS文件系统将会构建栅栏来阻止那些不再是组中的成员访问共享磁盘。
+ 磁盘容错
使用双层级别(dual-level)的RAID来进行磁盘容错控制。他能够识别出某个物理磁盘的错误或者缺乏某条路径访问某个磁盘。作为RAID一种补充，GPFS支持副本集，GPFS文件系统将会把数据和元数据，在两个不同的磁盘上保存。

**5 可扩展的系统工具**

GPFS文件系统可以允许增加，伸缩，重新组织一个文件系统通过adding,deleting或者更换现有文件系统中的磁盘。例如，当增加磁盘的时候，GPFS通过移动现有的文件到新的磁盘上而重新平衡文件系统；当移除或替换磁盘的时候，GPFS文件系统首先要将受影响的磁盘中所有的数据和元数据移出来，所有的这些操作都涉及到所有的inode和间接块。GPFS任命某个节点为文件系统管理节点，用它来负责管理活动，诸如协调文件系统。文件系统工具，使用一般的分布式锁来同步文件之间的活动。

**6 使用经验**

节点内和节点间的并发对于集群内部节点负载均衡同样重要。在初始系统设计的时候，以为集群中每个节点开一个线程就可以胜任处理分布式工作，并且足够利用可用的磁盘带宽。后来发现大量的磁盘IO请求，是的所有的磁盘满负荷。改进措施：一次IO处理，分配一个线程。节点内的并行性比起节点间的并行性，经常是一种更高效的改善性能的方式。节点内部使用多线程可以极大的减少工作负荷的偏斜。
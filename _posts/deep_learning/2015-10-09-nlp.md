---
layout: post
category: deep_learning
title: NLP
date: 2015-10-09
---

* TOC
{:toc}

# Tutorials

![](/assets/deep_learning/NLP/Deep Learning For NLP.jpg)

# Neural Models

**Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models**

<img src="/assets/dl-materials/Unifying_Visual-Semantic_Embeddings_with_Multimodal_Neural_Language_Models.png"/>

- paper: [http://arxiv.org/abs/1411.2539](http://arxiv.org/abs/1411.2539)
- results: [http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html](http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html)
- demo: [http://deeplearning.cs.toronto.edu/i2t](http://deeplearning.cs.toronto.edu/i2t)
- github: [https://github.com/ryankiros/visual-semantic-embedding](https://github.com/ryankiros/visual-semantic-embedding)

**Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks**

- arXiv: [http://arxiv.org/abs/1503.00075](http://arxiv.org/abs/1503.00075)
- github: [https://github.com/stanfordnlp/treelstm](https://github.com/stanfordnlp/treelstm)

**Visualizing and Understanding Neural Models in NLP**

- arXiv: [http://arxiv.org/abs/1506.01066](http://arxiv.org/abs/1506.01066)

**Character-Aware Neural Language Models**

- paper: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)
- github: [https://github.com/yoonkim/lstm-char-cnn](https://github.com/yoonkim/lstm-char-cnn)

**Skip-Thought Vectors**

- paper: [http://arxiv.org/abs/1506.06726](http://arxiv.org/abs/1506.06726)
- github: [https://github.com/ryankiros/skip-thoughts](https://github.com/ryankiros/skip-thoughts)

**A Primer on Neural Network Models for Natural Language Processing**

- arXiv: [http://arxiv.org/abs/1510.00726](http://arxiv.org/abs/1510.00726)

**Character-aware Neural Language Models**

- arxiv: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)

# Sequence to Sequence Learning

**Generating Text with Deep Reinforcement Learning(NIPS 2015)**

- arXiv: [http://arxiv.org/abs/1510.09202](http://arxiv.org/abs/1510.09202)

**MUSIO: A Deep Learning based Chatbot Getting Smarter**

- homepage: [http://ec2-204-236-149-143.us-west-1.compute.amazonaws.com:9000/](http://ec2-204-236-149-143.us-west-1.compute.amazonaws.com:9000/)
- github(Torch7): [https://github.com/deepcoord/seq2seq](https://github.com/deepcoord/seq2seq)

# Translation

**Learning phrase representations using rnn encoder-decoder for statistical machine translation**

- arXiv: [http://arxiv.org/abs/1406.1078](http://arxiv.org/abs/1406.1078)

**Neural Machine Translation by Jointly Learning to Align and Translate**

- arXiv: [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
- github: [https://github.com/lisa-groundhog/GroundHog](https://github.com/lisa-groundhog/GroundHog)

**Multi-Source Neural Translation**

- intro: "report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model."
- arxiv: [Multi-Source Neural Translation](Multi-Source Neural Translation)
- github(Zoph_RNN): [https://github.com/isi-nlp/Zoph_RNN](https://github.com/isi-nlp/Zoph_RNN)
- video: [http://research.microsoft.com/apps/video/default.aspx?id=260336](http://research.microsoft.com/apps/video/default.aspx?id=260336)

**Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism**

- arxiv: [http://arxiv.org/abs/1601.01073](http://arxiv.org/abs/1601.01073)

# Summarization

**A Neural Attention Model for Abstractive Sentence Summarization(EMNLP 2015. Facebook AI Research)**

- arXiv: [http://arxiv.org/abs/1509.00685](http://arxiv.org/abs/1509.00685)
- github: [https://github.com/facebook/NAMAS](https://github.com/facebook/NAMAS)

# Question Answering

**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks(Facebook AI Research)**

- arXiv: [http://arxiv.org/abs/1502.05698v1](http://arxiv.org/abs/1502.05698v1)
- github: [https://github.com/facebook/bAbI-tasks](https://github.com/facebook/bAbI-tasks)

**VQA: Visual Question Answering**

- arxiv: [http://arxiv.org/abs/1505.00468](http://arxiv.org/abs/1505.00468)
- homepage: [http://visualqa.org/](http://visualqa.org/)

**Ask Your Neurons: A Neural-based Approach to Answering Questions about Images**

- arxiv: [http://arxiv.org/abs/1505.01121](http://arxiv.org/abs/1505.01121)

**Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering**

- arxiv: [http://arxiv.org/abs/1505.05612](http://arxiv.org/abs/1505.05612)

**Teaching Machines to Read and Comprehend(Google DeepMind)**

- arXiv: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)
- github: [https://github.com/deepmind/rc-data](https://github.com/deepmind/rc-data)
- github(Theano/Blocks): [https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend](https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend)
- github(Tensorflow): [https://github.com/carpedm20/attentive-reader-tensorflow](https://github.com/carpedm20/attentive-reader-tensorflow)

**Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction**

![](http://cvlab.postech.ac.kr/research/dppnet/images/figure2.png)

- arxiv: [http://arxiv.org/abs/1511.05756](http://arxiv.org/abs/1511.05756)
- github: [https://github.com/HyeonwooNoh/DPPnet](https://github.com/HyeonwooNoh/DPPnet)
- project page: [http://cvlab.postech.ac.kr/research/dppnet/](http://cvlab.postech.ac.kr/research/dppnet/)

**Neural Generative Question Answering**

- arXiv: [http://arxiv.org/abs/1512.01337](http://arxiv.org/abs/1512.01337)

**Simple Baseline for Visual Question Answering (Facebook AI Research. Bag-of-word)**

- arXiv: [http://arxiv.org/abs/1512.02167](http://arxiv.org/abs/1512.02167)
- github: [https://github.com/metalbubble/VQAbaseline](https://github.com/metalbubble/VQAbaseline)
- demo: [http://visualqa.csail.mit.edu/](http://visualqa.csail.mit.edu/)

**MovieQA: Understanding Stories in Movies through Question-Answering**

- arxiv: [http://arxiv.org/abs/1512.02902](http://arxiv.org/abs/1512.02902)
- homepage: [http://movieqa.cs.toronto.edu/home/](http://movieqa.cs.toronto.edu/home/)

**Deeper LSTM+ normalized CNN for Visual Question Answering**

- intro: "This current code can get 58.16 on Open-Ended and 63.09 on Multiple-Choice on test-standard split"
- github: [https://github.com/VT-vision-lab/VQA_LSTM_CNN](https://github.com/VT-vision-lab/VQA_LSTM_CNN)

**A Neural Network for Factoid Question Answering over Paragraphs**

- project page: [http://cs.umd.edu/~miyyer/qblearn/](http://cs.umd.edu/~miyyer/qblearn/)
- paper: [https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf](https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf)
- code+data: [https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz](https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz)

**Generating Natural Questions About an Image**

- arxiv: [http://arxiv.org/abs/1603.06059](http://arxiv.org/abs/1603.06059)

**Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus**

- arxiv: [http://arxiv.org/abs/1603.06807](http://arxiv.org/abs/1603.06807)

# Alignment

**Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books**

- arXiv: [http://arxiv.org/abs/1506.06724](http://arxiv.org/abs/1506.06724)
- github: [https://github.com/ryankiros/neural-storyteller](https://github.com/ryankiros/neural-storyteller)

# Resources

**So, you need to understand language data? Open-source NLP software can help!**

![](http://entopix.com/assets/white-paper/slide1.png)

- blog: [http://entopix.com/so-you-need-to-understand-language-data-open-source-nlp-software-can-help.html](http://entopix.com/so-you-need-to-understand-language-data-open-source-nlp-software-can-help.html)